

\section{Finite elements simulation}
We will now shift our focus to a more general grid which is based on triangulation. In this section we will compare our parallel implementation from the previous section and disect the differences and similarities. For that reason we shall use the same sources in our grid as given in \texttt{sources.dat}. \\
Note that the sections for the exercises will be labeled as (2.1, 2.2, 2.3, ...) corresponding to exercises (4.1, 4.2, 4.3, ...) in the lab manual.\\
\subsection{Code understanding \& \texttt{Exchange\_Borders}}
The first step is to read through the code in \textbf{MPI\_Fempois.c} and to understand it. Furthermore, we have to implement the \texttt{Exchange\_Borders} function for which only a skeleton is given. The function should exchange the border values of the local grid with the neighboring processes.\\ 
The implementation of this function is quite straight forward. We only have to loop over all the neighbors of a process and send out the border values and receive the border values from the neighbors. The function is implemented as follows:

\begin{lstlisting}[language=c]
void Exchange_Borders(double *vect)
{
    for (size_t i = 0; i < N_neighb; ++i)
    {
        MPI_Sendrecv(vect, 1, send_type[i], proc_neighb[i], 0, //send
                     vect, 1, recv_type[i], proc_neighb[i], 0, //recv
                     grid_comm, &status);
    }
}
\end{lstlisting}

\subsection{Time benchmarking}
Next we turn our attention to timing of different sections in the code. \\
We have to measure: 
\begin{itemize}
    \item Time spent in computation 
    \item Time spent exchanging information with neighbors
    \item Time spent doing global communication
    \item Idle time
\end{itemize}
We setup the following variable to measure / deduce the time spent in the different sections:
\begin{lstlisting}[language=c]
    double total_time = 0.0;
    double exchange_time_neighbors = 0.0;
    double exchange_time_global = 0.0;
    double compute_time = 0.0;
\end{lstlisting}

We will measure the time spent in computations by timing the solve function and subtracting the time spent in the \texttt{MPI\_Allreduce} calls. The time spent in the \texttt{MPI\_Allreduce} calls is the time spent in global communication. The time spent in exchanging information with neighbors is the time spent in the \texttt{Exchange\_Borders} function. Finally, the idle time can be determined by summing up the differences between the cores total time and the slowest cores total time. \underline{Note} that this way of calculating the idle time is an approximation since it is assumed that the slowest core doesn't have any idle time. However, I've discussed this with the TA and he said that this is a valid way of calculating the idle time for this exercise.\\
The commandline output for runs is as shown in \autoref{fig:timing2} for an example run: 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{../fig/lab2/ex2.png}
    \caption{Timing for the different sections.\\
    (x) \dots denotes the process rank\\
    Compute time \dots time spent in the \texttt{solve} function \underline{only on computing}\\
    Exchange time (neighbors) \dots time spent in \texttt{Exchange\_Borders} \\
    Exchange time (neighbors) \dots time spent in \texttt{MPI\_Allreduce} calls \\
    Sum of times \dots total time spent in compute and communication (excluding setup / idle time)\\
    Total time \dots total time spent in the program\\
    }
    \label{fig:timing2}
\end{figure}

The \underline{idle time} is calculated as denoted above and therefore not shown in the output in \autoref{fig:timing2}. We get the following results from our Delft Blue runs in \autoref{tab:timing}:


\begin{table}[H]
    \centering
    \caption{Rank averaged time benchmark for different grid sizes and topologies.\\ All times are in milliseconds.\\\textbf{Note:} WTime does also include setup and teardown (mallocs, frees, etc.) - therefore the sum of the times is not equal to WTime.}
    \label{tab:timing}
    \begin{tabular}{|l|l|S[table-format=4.1]|S[table-format=4.1]|S[table-format=2.1]|S[table-format=2.1]|S[table-format=2.1]|}
        \hline
        Top. & {{{Grid Size}}} & {{{WTime (avg)}}} & {{{Comp. (avg)}}} & {{{Ex. Neighb. (avg)}}} & {{{Ex. Global (avg)}}} & {{{Idle (avg)}}} \\\hline
        1x4 & 100x100 & 82.0 & 18.6 & 1.3 & 1.7 & 9.0 \\\hline
        1x4 & 200x200 & 194.5 & 150.6 & 3.6 & 6.8 & 12.1 \\\hline
        1x4 & 400x400 & 1498.2 & 1357.8 & 10.1 & 26.4 & 9.1 \\\hline
        2x2 & 100x100 & 43.7 & 18.6 & 1.6 & 1.4 & 9.1 \\\hline
        2x2 & 200x200 & 184.0 & 145.2 & 4.3 & 8.1 & 5.7 \\\hline
        2x2 & 400x400 & 1428.7 & 1279.2 & 11.5 & 22.4 & 25.6 \\\hline
    \end{tabular}
\end{table}
We see that the total time is comparable between the two topologies for all grid sizes. Furthermore, the total runtime also increases non-surprisingly with the gridsize. \\
\Shining{Why does computation time increase faster than linearly?}\\
One could expect, that the grid with $200\times200$ elements would take 4 times as long as the grid with $100\times100$ elements. However, the runtime roughly 8 times longer. This happens because additionally to the higher number of grid points the algorithm also takes longer (more iterations) to converge and hence the computation time is longer. \\
\Shining{Analysis of the different times}\\
To get a better grasp on the data a stacked bar plot, as shown in \autoref{fig:timingbar}, is created. We immediately see that the bulk of the time is spent on computing the solution. The second biggest contributor is the global exchange time and the idle time, followed by local exchange time.\\
That computation takes up the biggest part of the time is of non surprise especially on the bigger grids this is to be expected. Rather surprisingly the global excahnge time takes up a lot of time. This begs the question why two lines of the form:
\begin{lstlisting}[language=c]
MPI_Allreduce(... , ... , 1, MPI_DOUBLE, MPI_SUM, grid_comm);
\end{lstlisting}
take up this much time. This actually comes down to masked idle time. While the operation itself only sums up the values of 4 double, the operation is blocking and therefore the other cores have to wait for the slowest core to finish. If we look back at the definition of idle time, we defined it as the difference of the total time of the slowest core and the total time of the current core. However, as stated above, this does not take waiting time in MPI calls (like \texttt{MPI\_Allreduce}) into account. Certainly, this explains now that the global exchange time is high, because every core has to wait for the others to synchronize to compute the \texttt{MPI\_Allreduce}.\\
\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../fig/lab2/average_times_stacked_bar_22.png}
    \end{minipage}%
    \hspace{0.02\textwidth}
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../fig/lab2/average_times_stacked_bar_no_comp_22.png}
    \end{minipage}
    \caption{Scaling behavior of the Poisson solver with different grid sizes and processor topologies}
    \label{fig:timingbar}
\end{figure}
We conclude that most of the time spent on communication in our solver is actually due to the waiting time in the \texttt{MPI\_Allreduce} calls and the other idle time (waiting for the slowest core to finish). The actual exchange of data only represents a fraction of this time. Biggest, especially for larger grids, contributor for the time is still the computation which takes up north of 95\% of the total time on larger grids.\\
\subsection{Data exchange amount}
This section is about the amount of data exchanged each iteration among one process with all its neighbors.\\
We assume a uniformly triangulated grid which is partitioned stripe-wise and distribution over $P$ processes. Furthermore, we assume that each process has to send the same amount of data. Indeed, this is not generally the case but for examples with periodic boundary conditions this is for example a valid assumption.\\
Now we see that every process has to communicate with $2d$ neighbors, where $d$ is the dimension of the grid, under our assumptions there are $2$ neighboring processes. With the assumption of a $n^2$ grid, our process communicates $n$ values with each of these neighbors. For striped partitioning we get: 
\begin{equation*}
    \text{Data exchanged per Process} = 2n 
\end{equation*}
or in total: 
\begin{equation*}
    \text{Data exchanged} = 2n \cdot P 
\end{equation*}
Let's check for the extreme case $1000\times1000$ grid and $P=500$ processes. We get:
\begin{equation*}
    \text{Data exchanged} = 2\cdot1000\cdot500 = 1000000
\end{equation*}
Which is the same amount of data as there are datapoints. This makes sense because every process has to communicate the top row upwards and the bottom row downwards.\\
Note, that we could even do worse if we assigned every process a single row. In this case every process would have to communicate the same data upward and downward. This would result in a data exchange of $2\cdot1000\cdot1000 = 2000000$ which is double the amount of data as there are datapoints.\\
For a box partitioning a process has to communicate $\nicefrac{n}{\sqrt{P}}$ (assuming divisibility with all neighbors) with each of its 4 neighbors. We get:
\begin{equation*}
    \text{Data exchanged per Process} = 4 \cdot \nicefrac{n}{\sqrt{P}}
\end{equation*}
or in total: 
\begin{equation*}
    \text{Data exchanged} = 4 \cdot n \cdot \sqrt{P}
\end{equation*}
This means that a striped partitioning is less efficient compared to a boxed partitioning starting from $P=4$, as can be seen in \autoref{fig:dataexchange}.\\
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{../fig/lab2/scale23.png}
    \caption{Striped vs. Boxed partitioning on a $100\times100$ grid using $P$ processors.}
    \label{fig:dataexchange}
\end{figure}
\subsection{Unbalanced communication}
In the last section we assumed that every process has to communicate with the same amount of neighbors in our given scenario. That is actually not entierly true. There is an imbalance, even if just a small one.\\ 
\Shining{But where does this imbalance come from?}\\
Let's take a look at a \Shining{uniformly triangulated grid} with $P=4$ processes. We assume that the grid is box-partitioned as given in \autoref{fig:boxpart24}.\\
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{../fig/lab2/grid24.png}
    \caption{Box partitioning of a uniformly triangulated grid with $P=4$ processes.\\
    One point (big-blue) in $1$ (and similarly in $3$) has to communicate its value to 3 neighbors (small-blue).\\
    Another point (big-orange circle) in $2$ (and similarly in $4$) has to communicate its value to 2 neighbors (small-orange circles).}
    \label{fig:boxpart24}
\end{figure}
We see that the geometry of a given uniform triangulated grid leads to an imbalance in communication. This imbalance is usually not a big problem, especially for bigger grids with comparably small amounts of processors because only 2 points have to communicate with 3 neighbors. However, for smaller grids or proportionally high processor counts the imbalance is more significant.\\ 

Looking at a $3\times3$ grid with $P=9$ processors, we see that the imbalance is more significant as shown in \autoref{fig:boxpart243}.\\
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{../fig/lab2/grid_3x3_partition.png}
    \caption{Box partitioning of a uniformly triangulated grid with $P=9$ processes.\\
    Corner (0/8 - blue/red) processes have to communicate their values to 2 or 3 neighbors respectively (1 and 3 - blue or 4, 5 and 7 - red).\\
    Central (4 - orange) processes have to communicate their value to 6 neighbors (1, 3, 5, 6, 7 and 8).}
    \label{fig:boxpart243}
\end{figure}
As we can see the imbalance comes from the partitioning geometry on the uniform triangulated grid. Because our processor grid is rectangular the results will always, in one way or another come out as shown in the schematic above.\\
For $3\times3$ grid we have 4 corner processes which have to communicate with 2 or 3 neighbors and 1 central process which has to communicate with 6 neighbors. This is a significant imbalance and compared to the $2\times2$ grid it is not negligible, because the central process has to communicate more than 2 extra points (whole edges of datapoints) to its neighbors.\\
\subsection{Estimates for computation $\equiv$ communication time}
\Shining{N.B.} This section will use two different ways to estimate the equilibrium point.\\

1) Let's assume that we have 4 processes and want to estimate for which grid size the computation time is equal to the communication time. 
One way to estimate this is by fitting a polynomial of degree 2 to the computation and a linear fit to the exchange time for our measurements in \autoref{tab:timing} and find the point where the times are equal. Thereafter we can use this information to determine the grid size where this happens. This process gives us a \underline{value of $n\approx84$} for the grid size as shown in \autoref{fig:comp_eq_comm}\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{../fig/lab2/estimate25.png}
    \caption{Fitting expected computation time $\equiv$ communication time for 4 processes.}
    \label{fig:comp_eq_comm}
\end{figure}

2) We'll also estimate this equilibrium point for a $1000\times1000$ grid. However, this time around we'll actually use a different way of calculating the number of Processes. We use the definitions from the lecture for stencil type computation: 
\begin{itemize}
    \item Computation time: $T_{\text{S}} = \text{number\_ops} \cdot t_\text{op} \sim  4 t_\text{op} \cdot n^2 \hspace{5mm} \text{(for stencil type - overall)}$
    \item Communication time: $T_{\text{comm}} = \text{number\_comm} \cdot t_\text{comm} \sim 2 n \cdot t_\text{data}\hspace{5mm} \text{(for stencil type - overall)}$
\end{itemize}
Note that the definitions above \underline{apply for a single iteration and \textbf{one} processor!}\\
We can calculate the time for communication from the data in \autoref{tab:timing} as follows: 
\[t_\text{data}(P) = \frac{t_\text{global\_comm}+t_\text{neighbor\_comm}}{2\cdot n \cdot P \cdot \#_\text{{iterations}}}\]
Similarly we can calculate the time for computation as follows:
\[t_\text{op} = \frac{t_\text{comp.}}{4 \cdot n^2 \cdot \#_\text{{iterations}}}\]
Note that the later doesn't depend on the number of processes.\\
Using the data from \autoref{tab:timing} we determine that one operation takes $t_\text{op} = \SI{3.3(3)e-6}{\milli\second}$. From that we can determine the computation time $t_{\text{comp.}}$ for $n=1000$ as follows:
\[t_{\text{comp.}} = 4 \cdot n^2 \cdot t_\text{op} \cdot \#_\text{{iterations}} = 4 \cdot 1000^2 \cdot 3.3(3) \cdot 10^{-6} \cdot 1241 = \SI{17.8(15)}{\second}\]
where 1241 ($\#_\text{{iterations}}$) is the number of iterations to converge. We set $t_{\text{comp.}} = t_{\text{data}}$ and solve for $P$. We need values for $t_\text{global\_comm}$ and $t_\text{neighbor\_comm}$ and take a look at \autoref{fig:comp_eq_comm} and decide to further investigate the trend for the excahnge time ($= t_\text{global\_comm} + t_\text{neighbor\_comm}$). We decide to use a linear extrapolation as shown in \autoref{fig:comp_sencil_extra}. We get an estimated communication time of $\SI{100}{\milli\second}$. \\

Now we have everything to solve for P and using 1241 for $\#_\text{{iterations}}$ again we get $P = \num{29(3)}$ processes. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{../fig/lab2/estimate25_stencil.png}
    \caption{Extrapolating the communication time for $n=1000$ using a linear fit.}
    \label{fig:comp_sencil_extra}
\end{figure}

Our predictions are in: a $84\times84$ grid for 4 processes with equal computation and communication times and a $1000\times1000$ grid for 29 processes (also having equal computation and communication time).\\
We measure the ratio \nicefrac{$t_{\text{comp}}$}{$t_{\text{comm}}$} and get the following results for a $4\times1$ topology in \autoref{tab:comp_comm_ratio}:

\begin{table}[H]
    \centering
    \caption{Ratio of computation to communication time for different grid sizes.}
    \label{tab:comp_comm_ratio}
    \begin{tabular}{|c|S[table-format=1.2(1.2)]|}
        \hline
        {{{Grid Size}}} & {Ratio} \\\hline
        10x10 & \num{0.53(0.06)} \\\hline
        20x20 & \num{0.84(0.15)} \\\hline
        30x30 & \num{0.86(0.13)} \\\hline
        40x40 & \num{1.97(0.24)} \\\hline
        50x50 & \num{2.66(0.61)} \\\hline
        75x75 & \num{4.37(1.00)} \\\hline
        84x84 & \num{3.70(0.85)} \\\hline
        90x90 & \num{6.06(1.94)} \\\hline
    \end{tabular}
\end{table}
We see that our initial guess of $84\times84$ grid for 4 processes is off by a long-shot. However, we also notice that the ratio is highly volatile as can be seen by the high uncertainties on the ratio. A rerun of the code produced notably different ratios (e.g. $\num{1.35(25)}$ for $30\times30$ grid). It seems that the actual position of the equilibrium point is around $n=35$. As a physicist I have to say that our initial guess was of the same magnitude and therefore not too far off \smiley.\\

Let's see how our prediction for the $1000\times1000$ grid with 29 processes holds up. Using a $29\times1$ topology we get $\num{4.04(1.83)}$ for the ratio. It seems we have made a lower prediction in terms of $P$ for the equilibrium point. However, the ratio is in the same ballpark as the ratio for the $84\times84$ grid which is not surprising since we used the same underlying data to make the prediction.\\
\subsection{Adaptive gird}
We will now make our grid denser in the source point areas. \texttt{GridDist} already implements an argument (\textit{adapt}) for that purpose. In order to gauge the speed of convergence we let process 0 print the current precision of the solution. We perform runs with a reference and a denser grid to compare the convergence speed, convergence count and amount of computing time for a $2\times2$ topology and a $100\times100$, $200\times200$ and $400\times400$ grids.\\
We first take a look at the iterations needed for convergence using the standard percision goal of $0.0001$ we get the results in \autoref{tab:adapt_it_count}.\\
It becomes clear that the adaptive grid needs a few more iterations to converge. This is most likely due to the fact that the dense region near the sources initially lead to a slower \glq spreading\grq\, of the solution near the source.\\
\begin{table}[H]
    \centering
    \caption{Iterations needed for convergence on $2\times2$ topology for different grid sizes with and without adapt keyword.}
    \label{tab:adapt_it_count}
    \begin{tabular}{|c|c|c|c|}
        \hline
        {{{Grid Size}}} & {100x100} & {200x200} & {400x400} \\\hline
        {Iterations Reference} & 141 & 274 & 529 \\\hline
        {Iterations Adapt} &     146 & 278 & 532 \\\hline
    \end{tabular}
\end{table}
\Shining{Does such a distored grid lead to faster convergence?}\\
We can answer this questin with a clear no, at least in terms of iterations. Next we'll look at the time it takes to converge.\\

A bar-plot of the time till convergence is shown in \autoref{fig:adapt_time}. We see that the time till convergence is roughly the same for the reference and the adaptive grid. This is not surprising since the time till convergence is mainly determined by the number of iterations needed. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{../fig/lab2/adapt_time.png}
    \caption{Time till convergence (with setup) on $2\times2$ topology for different grid sizes with and without adapt keyword.}
    \label{fig:adapt_time}
\end{figure}
\Shining{Does it affect the speed of convergence?}\\
Also no for this one. The time for the reference / adaptive grid is roughly the same with one exception for the $100\times100$ grid.\\
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{../fig/lab2/adapt_stacked_time.png}
    \caption{Time till convergence (without setup) on $2\times2$ topology for different grid sizes with and without adapt keyword.}
    \label{fig:adapt_time2}
\end{figure}
If we stack the time needed for computation and communication we see that the computation as well as the communication time is roughly the same for the reference and the adaptive grid as shown in \autoref{fig:adapt_time2}.\\

\Shining{Does it affect the amount of computing time?}\\
So again no, the amount of computing time is roughly the same for the reference and the adaptive grid.\\

